{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------The metrics for Test Method A are--------\n",
      "Accuracy:0.8000\n",
      "Sensitivity:1.0000\n",
      "Specificity:0.6667\n",
      "Positive Predicted Value:0.6667\n",
      "Negative Predicted Value:1.0000\n",
      "\n",
      "----------The metrics for Test Method B are--------\n",
      "Accuracy:0.6000\n",
      "Sensitivity:0.5000\n",
      "Specificity:0.6667\n",
      "Positive Predicted Value:0.5000\n",
      "Negative Predicted Value:0.6667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def calculate_metrics(confusion_matrix, metrics=\"all\"):\n",
    "    \n",
    "    TP = confusion_matrix[1][1]\n",
    "    TN = confusion_matrix[0][0]\n",
    "    FP = confusion_matrix[0][1]\n",
    "    FN = confusion_matrix[1][0]\n",
    "    \n",
    "    #accuracy \n",
    "    acc = ( float (TP + TN) / float (TP + TN + FP + FN))\n",
    "    \n",
    "    #sensitivity\n",
    "    sensitivity = ( TP / float ( TP + FN))\n",
    "    \n",
    "    #specificity\n",
    "    specificity = ( TN / float (TN + FP))\n",
    "    \n",
    "    #Positive Predicted Value\n",
    "    ppv = ( TP /  float ( TP + FP))\n",
    "    \n",
    "    #Negative Predicted Value\n",
    "    npv = (TN / float ( TN + FN))  \n",
    "    \n",
    "    print(f\"Accuracy:{acc:.4f}\")\n",
    "    print(f\"Sensitivity:{sensitivity:.4f}\")\n",
    "    print(f\"Specificity:{specificity:.4f}\")\n",
    "    print(f\"Positive Predicted Value:{ppv:.4f}\")\n",
    "    print(f\"Negative Predicted Value:{npv:.4f}\")\n",
    "      \n",
    "data = {\n",
    "    'Sample ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Gold standard': ['yes', 'no', 'no', 'no', 'yes', 'no', 'no', 'yes', 'no', 'yes'],\n",
    "    'Test Method A': ['+', '-', '-', '+', '+', '-', '+', '+', '-', '+'],\n",
    "    'Test Method B': ['+', '-', '-', '-', '-', '-', '+', '-', '+', '+']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df['Gold standard'] = df['Gold standard'].map({'yes':1,'no':0})\n",
    "df['Test Method A'] = df['Test Method A'].map({'+':1, '-':0})\n",
    "df['Test Method B'] = df['Test Method B'].map({'+':1, '-':0})\n",
    "\n",
    "\n",
    "\n",
    "cm_a  = confusion_matrix(df['Gold standard'],df['Test Method A'])\n",
    "cm_b = confusion_matrix(df['Gold standard'], df['Test Method B'])\n",
    "#ConfusionMatrixDisplay(cm_a).plot()\n",
    "#ConfusionMatrixDisplay(cm_b).plot()\n",
    "\n",
    "print(\"\\n----------The metrics for Test Method A are--------\")\n",
    "calculate_metrics(cm_a)\n",
    "print(\"\\n----------The metrics for Test Method B are--------\")\n",
    "calculate_metrics(cm_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
